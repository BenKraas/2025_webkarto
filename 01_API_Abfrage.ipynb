{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b2666d7",
   "metadata": {},
   "source": [
    "# API Request Example to VRR\n",
    "Define the name of the API endpoint you want to query and the parameters you need to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a2472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import uuid\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize logging to log to both file and console\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('api_requests.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_api_request(datetime_dt, place_dm, name_dm):\n",
    "\n",
    "    # Function logs the API response status and handles different HTTP status codes\n",
    "    def communicate_response(response, place_dm, name_dm, datetime_dt):\n",
    "        \"\"\"Handles the response from the API and logs the status.\"\"\"\n",
    "        if response.status_code == 200:\n",
    "            logging.info(f\"(200) Request successful for {place_dm} {name_dm} at {datetime_dt.isoformat()}\")\n",
    "        elif response.status_code == 204:\n",
    "            logging.info(f\"(204) No departures found for {place_dm} {name_dm} at {datetime_dt.isoformat()}\")\n",
    "        elif response.status_code == 400:\n",
    "            logging.warning(f\"(400) Bad request for {place_dm} {name_dm} at {datetime_dt.isoformat()}\")\n",
    "        elif response.status_code == 404:\n",
    "            logging.error(f\"(404) Not found for {place_dm} {name_dm} at {datetime_dt.isoformat()}\")\n",
    "        elif response.status_code == 500:\n",
    "            logging.error(f\"(500) Internal server error for {place_dm} {name_dm} at {datetime_dt.isoformat()}\")\n",
    "        elif response.status_code == 503:\n",
    "            logging.error(f\"(503) Service unavailable for {place_dm} {name_dm} at {datetime_dt.isoformat()}\")\n",
    "        elif response.status_code == 429:\n",
    "            logging.error(f\"(429) Too many requests for {place_dm} {name_dm} at {datetime_dt.isoformat()}\")\n",
    "        else:\n",
    "            logging.error(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return response\n",
    "\n",
    "    # Function to create a unique identifier (UUID) for each departure\n",
    "    def make_uid(stop, scheduled_datetime, line):\n",
    "        base = f\"{stop}|{scheduled_datetime}|{line}\"\n",
    "        return str(uuid.uuid5(uuid.NAMESPACE_DNS, base))\n",
    "    \n",
    "    # Function to build the results from the API response\n",
    "    def build_results(datetime_dt, make_uid, departures):\n",
    "        results = []\n",
    "        for dep in departures:\n",
    "            stop_name = dep.get('stopName')\n",
    "            platform = dep.get('platformName', dep.get('platform'))\n",
    "            scheduled = dep.get('dateTime', {})\n",
    "            real = dep.get('realDateTime', {})\n",
    "            # Build full datetime for scheduled and real departure\n",
    "            try:\n",
    "                scheduled_dt = datetime(\n",
    "                    int(scheduled.get('year', datetime_dt.year)),\n",
    "                    int(scheduled.get('month', datetime_dt.month)),\n",
    "                    int(scheduled.get('day', datetime_dt.day)),\n",
    "                    int(scheduled.get('hour', 0)),\n",
    "                    int(scheduled.get('minute', 0))\n",
    "                )\n",
    "            except Exception:\n",
    "                scheduled_dt = None\n",
    "            try:\n",
    "                real_dt = datetime(\n",
    "                    int(real.get('year', datetime_dt.year)),\n",
    "                    int(real.get('month', datetime_dt.month)),\n",
    "                    int(real.get('day', datetime_dt.day)),\n",
    "                    int(real.get('hour', 0)),\n",
    "                    int(real.get('minute', 0))\n",
    "                ) if real else None\n",
    "            except Exception:\n",
    "                real_dt = None\n",
    "\n",
    "            line = dep.get('servingLine', {}).get('number')\n",
    "            direction = dep.get('servingLine', {}).get('direction')\n",
    "            delay = dep.get('servingLine', {}).get('delay')\n",
    "            cancelled = dep.get('servingLine', {}).get('cancelled')\n",
    "            connection_exists = not (str(cancelled) == \"1\")\n",
    "            # Additional fields\n",
    "            delay_reason = dep.get('servingLine', {}).get('delayReason')\n",
    "            realtime_status = dep.get('servingLine', {}).get('realtimeStatus')\n",
    "            status_text = dep.get('servingLine', {}).get('statusText')\n",
    "\n",
    "            uid = make_uid(stop_name, scheduled_dt, line)\n",
    "\n",
    "            results.append({\n",
    "                'uuid': uid,\n",
    "                'stop': stop_name,\n",
    "                'platform': platform,\n",
    "                'line': line,\n",
    "                'direction': direction,\n",
    "                'scheduled_departure': scheduled_dt,\n",
    "                'real_departure': real_dt,\n",
    "                'scheduled_time': scheduled_dt.time() if scheduled_dt else None,\n",
    "                'scheduled_date_iso': scheduled_dt.date().isoformat() if scheduled_dt else None,\n",
    "                'delay_min': int(delay) if delay not in (None, '', '-9999') else None,\n",
    "                'connection_exists': connection_exists,\n",
    "                'delay_reason': delay_reason,\n",
    "                'realtime_status': realtime_status,\n",
    "                'status_text': status_text\n",
    "            })\n",
    "            \n",
    "        return results\n",
    "    \n",
    "\n",
    "    # Prepare the parameters for the API request\n",
    "    params = {\n",
    "        \"language\": \"de\",\n",
    "        \"mode\": \"direct\",\n",
    "        \"outputFormat\": \"JSON\",\n",
    "        \"type_dm\": \"stop\",\n",
    "        \"useProxFootSearch\": 0,\n",
    "        \"useRealtime\": 1,\n",
    "        \"itdDateDay\": datetime_dt.day,\n",
    "        \"itdDateMonth\": datetime_dt.month,\n",
    "        \"itdDateYear\": datetime_dt.year,\n",
    "        \"itdTimeHour\": datetime_dt.hour,\n",
    "        \"itdTimeMinute\": datetime_dt.minute,\n",
    "        \"place_dm\": place_dm,\n",
    "        \"name_dm\": name_dm,\n",
    "    }\n",
    "\n",
    "    # Create a text file to store the raw API responses (Debugging purposes)\n",
    "    textfile = Path(\"vrr_api_full_responses.txt\")\n",
    "    if not textfile.exists():\n",
    "        textfile.touch()\n",
    "\n",
    "    # API URL for the VRR (Verkehrsverbund Rhein-Ruhr) departures\n",
    "    # This URL is used to fetch the departure information based on the parameters provided\n",
    "    # The API is expected to return a JSON response with the departure details\n",
    "    API_URL = \"https://efa.vrr.de/standard/XML_DM_REQUEST\"\n",
    "\n",
    "    # Make the API request\n",
    "    logging.info(f\"Making API request for {place_dm} {name_dm} at {datetime_dt.isoformat()}\")\n",
    "    response = requests.get(API_URL, params=params)\n",
    "    \n",
    "    # Handle the response\n",
    "    response = communicate_response(response, place_dm, name_dm, datetime_dt)\n",
    "\n",
    "    # Check if the response is successful and contains data\n",
    "    if response.status_code in [200, 204]:\n",
    "        # Write the raw response to a text file for debugging purposes\n",
    "        try:\n",
    "            with open(textfile, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(response.text + \"\\n\\n\")\n",
    "            logging.info(f\"Response written to {textfile}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error writing to {textfile}: {e}\")\n",
    "        data = response.json()\n",
    "    else:\n",
    "        # If the response is not successful, return an empty DataFrame and the status code\n",
    "        logging.error(f\"Failed to fetch data for {place_dm} {name_dm} at {datetime_dt.isoformat()}\")\n",
    "        raise requests.exceptions.RequestException(\n",
    "            f\"Request failed with status code {response.status_code} for {place_dm} {name_dm} at {datetime_dt.isoformat()}\"\n",
    "        )\n",
    "\n",
    "    # Extract the departure list from the response data\n",
    "    departures = data.get('departureList', [])\n",
    "\n",
    "    # Build the results from the departures\n",
    "    df_departures = pd.DataFrame(build_results(datetime_dt, make_uid, departures))\n",
    "\n",
    "    # LUT (Lookup Table) for replacing special characters in the stop, direction, and line names\n",
    "    # This is necessary to ensure that the data is clean and consistent, in this case for German characters\n",
    "    lut = {\"Ã¼\": \"ü\", \"Ã¶\": \"ö\", \"Ã¤\": \"ä\", \"ÃŸ\": \"ß\", \"Ã\": \"ß\"}\n",
    "    for col in ['stop', 'direction', 'line']:\n",
    "        df_departures[col] = df_departures[col].replace(lut, regex=True)\n",
    "\n",
    "    # Convert the scheduled and real departure times to ISO format\n",
    "    df_departures['scheduled_departure'] = pd.to_datetime(df_departures['scheduled_departure'], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    df_departures['real_departure'] = pd.to_datetime(df_departures['real_departure'], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    \n",
    "    # return df_departures, response.status_code\n",
    "    return df_departures, response.status_code\n",
    "\n",
    "def update_geodata(csv_file_path, geodata_file_path, geodata_target):\n",
    "    \"\"\"\n",
    "    For each stop in the DataFrame, get the last 10 departures and add them as lists into the GeoDataFrame.\n",
    "    The GeoDataFrame uses short column names due to shapefile limitations, so columns are mapped accordingly.\n",
    "    \"\"\"\n",
    "    # Mapping from long names (df) to short names (gdf)\n",
    "    col_map = {\n",
    "        'stop': 'stop',\n",
    "        'line': 'line',\n",
    "        'direction': 'direct',\n",
    "        'scheduled_departure': 'schedudep',\n",
    "        'real_departure': 'realdep',\n",
    "        'delay_min': 'delay'\n",
    "    }\n",
    "\n",
    "    # Load the existing GeoDataFrame\n",
    "    try:\n",
    "        gdf = gpd.read_file(geodata_file_path)\n",
    "        # Ensure columns are short-named in gdf\n",
    "        gdf.rename(columns=col_map, inplace=True)\n",
    "        logging.info(f\"Loaded existing geodata with {len(gdf)} entries. Columns: {gdf.columns.tolist()}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading geodata: {e}\")\n",
    "        return\n",
    "\n",
    "    # Load the CSV file into a DataFrame but only the last 200 rows\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path, nrows=200)\n",
    "        # Keep only the relevant columns\n",
    "        df = df[list(col_map.keys())]\n",
    "        # Rename columns to match the GeoDataFrame\n",
    "        df.rename(columns=col_map, inplace=True)\n",
    "        logging.info(f\"Loaded CSV with {len(df)} entries. Columns: {df.columns.tolist()}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"CSV file {csv_file_path} not found.\")\n",
    "        return\n",
    "\n",
    "    # Ensure the 'stop' column exists in the DataFrame\n",
    "    assert 'stop' in df.columns, \"The DataFrame does not contain a 'stop' column.\"\n",
    "\n",
    "    # for each stop in the DataFrame, get the last 10 departures, make each value into a list and add them to the GeoDataFrame merging on the 'stop' column\n",
    "    for stop in df['stop'].unique():\n",
    "        # Get the last 10 departures for the stop\n",
    "        stop_df = df[df['stop'] == stop].tail(10)\n",
    "\n",
    "        # Convert each column to a list\n",
    "        stop_departures = {\n",
    "            'stop': stop,\n",
    "            'line': stop_df['line'].tolist(),\n",
    "            'direct': stop_df['direct'].tolist(),\n",
    "            'schedudep': stop_df['schedudep'].tolist(),\n",
    "            'realdep': stop_df['realdep'].tolist(),\n",
    "            'delay': stop_df['delay'].tolist()\n",
    "        }\n",
    "\n",
    "        # Create a DataFrame from the stop_departures dictionary\n",
    "        stop_departures_df = pd.DataFrame([stop_departures])\n",
    "\n",
    "        # Add empty columns for any missing columns in the GeoDataFrame\n",
    "        for col in col_map.values():\n",
    "            if col not in stop_departures_df.columns:\n",
    "                stop_departures_df[col] = None\n",
    "\n",
    "        # Merge with the GeoDataFrame on the 'stop' column\n",
    "        gdf = gdf.merge(stop_departures_df, on='stop', how='left')\n",
    "    \n",
    "    # Save the updated GeoDataFrame back to the shapefile\n",
    "    try:\n",
    "        gdf.to_file(geodata_target, driver='ESRI Shapefile')\n",
    "        logging.info(f\"Updated geodata saved to {geodata_target}.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving updated geodata: {e}\")\n",
    "        return\n",
    "\n",
    "\n",
    "# Main function to handle the API requests and manage the CSV file\n",
    "def main(delay_min, placename_list):\n",
    "    total_requests = len(placename_list)\n",
    "    delay_s = delay_min * 60  # convert minutes to seconds\n",
    "    request_delay = delay_s / total_requests # time the actual requests so that they space out over the delay time\n",
    "\n",
    "    # initialize csv\n",
    "    csv_file = Path('final_departures.csv')\n",
    "    geodata_file = Path('res/geodata/bahnhoefe.shp')\n",
    "    geodata_target = Path('data/geodata/bahnhoefe_running.shp')\n",
    "\n",
    "    # Main loop\n",
    "    logging.info(f\"Total requests: {total_requests}, Delay per request: {round(request_delay/60, 2)} minutes.\")\n",
    "    logging.info(\"Starting the request loop...\")\n",
    "\n",
    "    # Load existing UUIDs only once at the start\n",
    "    try:\n",
    "        existing_df = pd.read_csv(csv_file, usecols=['uuid'])\n",
    "        existing_uuids = set(existing_df['uuid'].dropna().astype(str))\n",
    "        logging.info(f\"Loaded {len(existing_uuids)} existing UUIDs.\")\n",
    "    except FileNotFoundError:\n",
    "        existing_uuids = set()\n",
    "        logging.info(\"No existing UUIDs found, starting fresh.\")\n",
    "\n",
    "    while True:\n",
    "        logging.info(\"Starting a new cycle of requests...\")\n",
    "\n",
    "        for place_dm, name_dm in placename_list:\n",
    "            try:\n",
    "                datetime_dt = datetime.now()\n",
    "\n",
    "                df, status_code = full_api_request(datetime_dt, place_dm, name_dm)\n",
    "\n",
    "                if not df.empty:\n",
    "                    df['uuid'] = df['uuid'].astype(str)\n",
    "                    new_df = df[~df['uuid'].isin(existing_uuids)]\n",
    "\n",
    "                    if not new_df.empty:\n",
    "                        new_df.to_csv(csv_file, mode='a', header=not existing_uuids, index=False)\n",
    "                        existing_uuids.update(new_df['uuid'])\n",
    "\n",
    "                        # Update the geodata with the new departures\n",
    "                        update_geodata(csv_file, geodata_file, geodata_target)\n",
    "\n",
    "                        logging.info(f\"Appended {len(new_df)} new departures. Status code: {status_code}\")\n",
    "                    else:\n",
    "                        logging.info(\"No new UUIDs to append.\")\n",
    "                else:\n",
    "                    logging.info(f\"No departures found for {place_dm} - {name_dm}. Status code: {status_code}\")\n",
    "\n",
    "                logging.info(f\"Sleeping for {round(request_delay/60, 2)} minutes.\")\n",
    "                time.sleep(request_delay)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logging.error(f\"Request failed for {place_dm} - {name_dm} ({status_code}): {e}\")\n",
    "                time.sleep(request_delay)\n",
    "                continue\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"An error occurred while processing {place_dm} - {name_dm}: {e}\")\n",
    "                time.sleep(request_delay)\n",
    "                continue\n",
    "\n",
    "        logging.info(\"Next cycle...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb1716",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (148001247.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# Define the datetime for the request\n",
    "return\n",
    "datetime_dt = datetime.now()\n",
    "\n",
    "# Define the place and name for the stop\n",
    "place_dm = \"Gelsenkirchen\"\n",
    "name_dm = \"HBF\"\n",
    "\n",
    "placename_list = [(\"Duisburg\", \"HBF\"), (\"Mönchengladbach\", \"HBF\"), (\"Wuppertal\", \"HBF\"), (\"Bochum\", \"HBF\"), (\"Dortmund\", \"HBF\"), (\"Essen\", \"HBF\"), (\"Düsseldorf\", \"HBF\")]\n",
    "placename_list = [(\"Duisburg\", \"HBF\")]\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "# Make the API request and get the DataFrame\n",
    "for place_dm, name_dm in placename_list:\n",
    "    print(f\"Requesting data for {place_dm} - {name_dm}\")\n",
    "    df, status_code = full_api_request(datetime_dt, place_dm, name_dm)\n",
    "    if not df.empty:\n",
    "        final_df = pd.concat([final_df, df], ignore_index=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35232bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 13:23:36,441 - INFO - Total requests: 7, Delay per request: 0.07 minutes.\n",
      "2025-07-12 13:23:36,441 - INFO - Starting the request loop...\n",
      "2025-07-12 13:23:36,450 - INFO - Loaded 749 existing UUIDs.\n",
      "2025-07-12 13:23:36,451 - INFO - Starting a new cycle of requests...\n",
      "2025-07-12 13:23:36,452 - INFO - Making API request for Duisburg HBF at 2025-07-12T13:23:36.452454\n",
      "2025-07-12 13:23:36,859 - INFO - (200) Request successful for Duisburg HBF at 2025-07-12T13:23:36.452454\n",
      "2025-07-12 13:23:36,860 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:23:36,897 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:23:36,898 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:23:41,184 - INFO - Making API request for Mönchengladbach HBF at 2025-07-12T13:23:41.184715\n",
      "2025-07-12 13:23:41,557 - INFO - (200) Request successful for Mönchengladbach HBF at 2025-07-12T13:23:41.184715\n",
      "2025-07-12 13:23:41,559 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:23:41,570 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:23:41,571 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:23:45,858 - INFO - Making API request for Wuppertal HBF at 2025-07-12T13:23:45.858115\n",
      "2025-07-12 13:23:46,230 - INFO - (200) Request successful for Wuppertal HBF at 2025-07-12T13:23:45.858115\n",
      "2025-07-12 13:23:46,232 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:23:46,241 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:23:46,242 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:23:50,529 - INFO - Making API request for Bochum HBF at 2025-07-12T13:23:50.528906\n",
      "2025-07-12 13:23:50,970 - INFO - (200) Request successful for Bochum HBF at 2025-07-12T13:23:50.528906\n",
      "2025-07-12 13:23:50,972 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:23:50,983 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:23:50,984 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:23:55,271 - INFO - Making API request for Dortmund HBF at 2025-07-12T13:23:55.270954\n",
      "2025-07-12 13:23:55,603 - INFO - (200) Request successful for Dortmund HBF at 2025-07-12T13:23:55.270954\n",
      "2025-07-12 13:23:55,604 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:23:55,615 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:23:55,616 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:23:59,902 - INFO - Making API request for Essen HBF at 2025-07-12T13:23:59.902771\n",
      "2025-07-12 13:24:00,405 - INFO - (200) Request successful for Essen HBF at 2025-07-12T13:23:59.902771\n",
      "2025-07-12 13:24:00,407 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:24:00,458 - INFO - Loaded existing geodata with 7 entries. Columns: ['name', 'geometry']\n",
      "2025-07-12 13:24:00,467 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 13:25:08,078 - ERROR - An error occurred while processing Essen - HBF: 'stop'\n",
      "2025-07-12 13:25:12,368 - INFO - Making API request for Düsseldorf HBF at 2025-07-12T13:25:12.367266\n",
      "2025-07-12 13:25:12,778 - INFO - (200) Request successful for Düsseldorf HBF at 2025-07-12T13:25:12.367266\n",
      "2025-07-12 13:25:12,780 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:25:12,814 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'geometry']\n",
      "2025-07-12 13:25:12,822 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "/tmp/ipykernel_98361/2791269995.py:229: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  gdf.to_file(geodata_file_path, driver='ESRI Shapefile')\n",
      "/home/ben/.local/share/mamba/envs/2025_webkarto/lib/python3.13/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'schedudep_x' to 'schedudep_'\n",
      "  ogr_write(\n",
      "/home/ben/.local/share/mamba/envs/2025_webkarto/lib/python3.13/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'schedudep_y' to 'schedude_1'\n",
      "  ogr_write(\n",
      "/home/ben/.local/share/mamba/envs/2025_webkarto/lib/python3.13/site-packages/pyogrio/raw.py:723: RuntimeWarning: Value '['Mönchengl. Nellessenweg', 'Wegberg, Schwalmaue', 'Viersen Wegweiser', 'Willich St. Töniser Straße', 'Mönchengl. Breiter Graben', 'Mönchengl. Landscheidung', 'Mönchengl. Am Brückensteg', 'Mönchengl Liebfrauenstraße', 'MG Flughafen Terminal', 'Mönchengladbach Wanlo Markt']' of field direct_y has been truncated to 254 characters.  This warning will not be emitted any more for that layer.\n",
      "  ogr_write(\n",
      "2025-07-12 13:25:12,852 - INFO - Created 7 records\n",
      "2025-07-12 13:25:12,853 - INFO - Updated geodata saved to res/geodata/bahnhoefe.shp.\n",
      "2025-07-12 13:25:12,854 - INFO - Appended 6 new departures. Status code: 200\n",
      "2025-07-12 13:25:12,855 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:25:17,143 - INFO - Next cycle...\n",
      "2025-07-12 13:25:17,144 - INFO - Starting a new cycle of requests...\n",
      "2025-07-12 13:25:17,145 - INFO - Making API request for Duisburg HBF at 2025-07-12T13:25:17.145130\n",
      "2025-07-12 13:25:17,518 - INFO - (200) Request successful for Duisburg HBF at 2025-07-12T13:25:17.145130\n",
      "2025-07-12 13:25:17,519 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:25:17,534 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:25:17,538 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:25:17,542 - ERROR - An error occurred while processing Duisburg - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:25:21,829 - INFO - Making API request for Mönchengladbach HBF at 2025-07-12T13:25:21.829473\n",
      "2025-07-12 13:25:22,227 - INFO - (200) Request successful for Mönchengladbach HBF at 2025-07-12T13:25:21.829473\n",
      "2025-07-12 13:25:22,229 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:25:22,245 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:25:22,250 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:25:22,254 - ERROR - An error occurred while processing Mönchengladbach - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:25:26,542 - INFO - Making API request for Wuppertal HBF at 2025-07-12T13:25:26.541968\n",
      "2025-07-12 13:25:26,962 - INFO - (200) Request successful for Wuppertal HBF at 2025-07-12T13:25:26.541968\n",
      "2025-07-12 13:25:26,964 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:25:26,975 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:25:26,980 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:25:26,983 - ERROR - An error occurred while processing Wuppertal - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:25:31,270 - INFO - Making API request for Bochum HBF at 2025-07-12T13:25:31.270410\n",
      "2025-07-12 13:25:31,678 - INFO - (200) Request successful for Bochum HBF at 2025-07-12T13:25:31.270410\n",
      "2025-07-12 13:25:31,680 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:25:31,696 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:25:31,701 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:25:31,704 - ERROR - An error occurred while processing Bochum - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:25:35,991 - INFO - Making API request for Dortmund HBF at 2025-07-12T13:25:35.991429\n",
      "2025-07-12 13:25:36,322 - INFO - (200) Request successful for Dortmund HBF at 2025-07-12T13:25:35.991429\n",
      "2025-07-12 13:25:36,324 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:25:36,338 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:25:36,344 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:25:36,347 - ERROR - An error occurred while processing Dortmund - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:25:40,635 - INFO - Making API request for Essen HBF at 2025-07-12T13:25:40.634953\n",
      "2025-07-12 13:25:41,078 - INFO - (200) Request successful for Essen HBF at 2025-07-12T13:25:40.634953\n",
      "2025-07-12 13:25:41,080 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:25:41,094 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:25:41,100 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:25:41,103 - ERROR - An error occurred while processing Essen - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:25:45,391 - INFO - Making API request for Düsseldorf HBF at 2025-07-12T13:25:45.390947\n",
      "2025-07-12 13:25:46,233 - INFO - (200) Request successful for Düsseldorf HBF at 2025-07-12T13:25:45.390947\n",
      "2025-07-12 13:25:46,234 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:25:46,245 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:25:46,246 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:25:50,533 - INFO - Next cycle...\n",
      "2025-07-12 13:25:50,534 - INFO - Starting a new cycle of requests...\n",
      "2025-07-12 13:25:50,535 - INFO - Making API request for Duisburg HBF at 2025-07-12T13:25:50.535604\n",
      "2025-07-12 13:25:50,889 - INFO - (200) Request successful for Duisburg HBF at 2025-07-12T13:25:50.535604\n",
      "2025-07-12 13:25:50,891 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:25:50,900 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:25:50,901 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:25:55,188 - INFO - Making API request for Mönchengladbach HBF at 2025-07-12T13:25:55.187900\n",
      "2025-07-12 13:25:55,679 - INFO - (200) Request successful for Mönchengladbach HBF at 2025-07-12T13:25:55.187900\n",
      "2025-07-12 13:25:55,681 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:25:55,691 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:25:55,692 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:25:59,979 - INFO - Making API request for Wuppertal HBF at 2025-07-12T13:25:59.979393\n",
      "2025-07-12 13:26:00,331 - INFO - (200) Request successful for Wuppertal HBF at 2025-07-12T13:25:59.979393\n",
      "2025-07-12 13:26:00,333 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:26:00,346 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:26:00,351 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:26:00,355 - ERROR - An error occurred while processing Wuppertal - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:26:04,642 - INFO - Making API request for Bochum HBF at 2025-07-12T13:26:04.642052\n",
      "2025-07-12 13:26:05,033 - INFO - (200) Request successful for Bochum HBF at 2025-07-12T13:26:04.642052\n",
      "2025-07-12 13:26:05,035 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:26:05,050 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:26:05,055 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:26:05,059 - ERROR - An error occurred while processing Bochum - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:26:09,347 - INFO - Making API request for Dortmund HBF at 2025-07-12T13:26:09.346857\n",
      "2025-07-12 13:26:09,728 - INFO - (200) Request successful for Dortmund HBF at 2025-07-12T13:26:09.346857\n",
      "2025-07-12 13:26:09,730 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:26:09,745 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:26:09,751 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:26:09,756 - ERROR - An error occurred while processing Dortmund - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:26:14,044 - INFO - Making API request for Essen HBF at 2025-07-12T13:26:14.043944\n",
      "2025-07-12 13:26:14,480 - INFO - (200) Request successful for Essen HBF at 2025-07-12T13:26:14.043944\n",
      "2025-07-12 13:26:14,482 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:26:14,496 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:26:14,500 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:26:14,504 - ERROR - An error occurred while processing Essen - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:26:18,791 - INFO - Making API request for Düsseldorf HBF at 2025-07-12T13:26:18.790997\n",
      "2025-07-12 13:26:19,216 - INFO - (200) Request successful for Düsseldorf HBF at 2025-07-12T13:26:18.790997\n",
      "2025-07-12 13:26:19,219 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:26:19,235 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:26:19,239 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:26:19,243 - ERROR - An error occurred while processing Düsseldorf - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:26:23,530 - INFO - Next cycle...\n",
      "2025-07-12 13:26:23,531 - INFO - Starting a new cycle of requests...\n",
      "2025-07-12 13:26:23,532 - INFO - Making API request for Duisburg HBF at 2025-07-12T13:26:23.531992\n",
      "2025-07-12 13:26:23,885 - INFO - (200) Request successful for Duisburg HBF at 2025-07-12T13:26:23.531992\n",
      "2025-07-12 13:26:23,886 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:26:23,900 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:26:23,905 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:26:23,909 - ERROR - An error occurred while processing Duisburg - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:26:28,196 - INFO - Making API request for Mönchengladbach HBF at 2025-07-12T13:26:28.196631\n",
      "2025-07-12 13:26:28,583 - INFO - (200) Request successful for Mönchengladbach HBF at 2025-07-12T13:26:28.196631\n",
      "2025-07-12 13:26:28,585 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:26:28,606 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:26:28,614 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:26:28,618 - ERROR - An error occurred while processing Mönchengladbach - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:26:32,906 - INFO - Making API request for Wuppertal HBF at 2025-07-12T13:26:32.905965\n",
      "2025-07-12 13:26:33,275 - INFO - (200) Request successful for Wuppertal HBF at 2025-07-12T13:26:32.905965\n",
      "2025-07-12 13:26:33,277 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:26:33,291 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:26:33,296 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:26:33,301 - ERROR - An error occurred while processing Wuppertal - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:26:37,588 - INFO - Making API request for Bochum HBF at 2025-07-12T13:26:37.588296\n",
      "2025-07-12 13:26:37,897 - INFO - (200) Request successful for Bochum HBF at 2025-07-12T13:26:37.588296\n",
      "2025-07-12 13:26:37,899 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:26:37,912 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:26:37,912 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:26:42,199 - INFO - Making API request for Dortmund HBF at 2025-07-12T13:26:42.199634\n",
      "2025-07-12 13:26:46,004 - INFO - (200) Request successful for Dortmund HBF at 2025-07-12T13:26:42.199634\n",
      "2025-07-12 13:26:46,006 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:26:46,014 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:26:46,015 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:26:50,302 - INFO - Making API request for Essen HBF at 2025-07-12T13:26:50.302193\n",
      "2025-07-12 13:26:50,782 - INFO - (200) Request successful for Essen HBF at 2025-07-12T13:26:50.302193\n",
      "2025-07-12 13:26:50,783 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:26:50,793 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:26:50,794 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:26:55,081 - INFO - Making API request for Düsseldorf HBF at 2025-07-12T13:26:55.081565\n",
      "2025-07-12 13:26:55,497 - INFO - (200) Request successful for Düsseldorf HBF at 2025-07-12T13:26:55.081565\n",
      "2025-07-12 13:26:55,498 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:26:55,511 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:26:55,512 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:26:59,798 - INFO - Next cycle...\n",
      "2025-07-12 13:26:59,799 - INFO - Starting a new cycle of requests...\n",
      "2025-07-12 13:26:59,800 - INFO - Making API request for Duisburg HBF at 2025-07-12T13:26:59.800441\n",
      "2025-07-12 13:27:00,139 - INFO - (200) Request successful for Duisburg HBF at 2025-07-12T13:26:59.800441\n",
      "2025-07-12 13:27:00,140 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:27:00,150 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:27:00,151 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:27:04,438 - INFO - Making API request for Mönchengladbach HBF at 2025-07-12T13:27:04.438494\n",
      "2025-07-12 13:27:04,823 - INFO - (200) Request successful for Mönchengladbach HBF at 2025-07-12T13:27:04.438494\n",
      "2025-07-12 13:27:04,824 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:27:04,838 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:27:04,844 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:27:04,849 - ERROR - An error occurred while processing Mönchengladbach - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:27:09,135 - INFO - Making API request for Wuppertal HBF at 2025-07-12T13:27:09.135821\n",
      "2025-07-12 13:27:09,559 - INFO - (200) Request successful for Wuppertal HBF at 2025-07-12T13:27:09.135821\n",
      "2025-07-12 13:27:09,561 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:27:09,576 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:27:09,582 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:27:09,585 - ERROR - An error occurred while processing Wuppertal - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:27:13,872 - INFO - Making API request for Bochum HBF at 2025-07-12T13:27:13.872848\n",
      "2025-07-12 13:27:14,212 - INFO - (200) Request successful for Bochum HBF at 2025-07-12T13:27:13.872848\n",
      "2025-07-12 13:27:14,214 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:27:14,227 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:27:14,228 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:27:18,515 - INFO - Making API request for Dortmund HBF at 2025-07-12T13:27:18.515745\n",
      "2025-07-12 13:27:18,829 - INFO - (200) Request successful for Dortmund HBF at 2025-07-12T13:27:18.515745\n",
      "2025-07-12 13:27:18,830 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:27:18,850 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:27:18,857 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:27:18,860 - ERROR - An error occurred while processing Dortmund - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:27:23,148 - INFO - Making API request for Essen HBF at 2025-07-12T13:27:23.148025\n",
      "2025-07-12 13:27:23,605 - INFO - (200) Request successful for Essen HBF at 2025-07-12T13:27:23.148025\n",
      "2025-07-12 13:27:23,606 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:27:23,621 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:27:23,626 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:27:23,629 - ERROR - An error occurred while processing Essen - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:27:27,917 - INFO - Making API request for Düsseldorf HBF at 2025-07-12T13:27:27.916858\n",
      "2025-07-12 13:27:28,329 - INFO - (200) Request successful for Düsseldorf HBF at 2025-07-12T13:27:27.916858\n",
      "2025-07-12 13:27:28,331 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:27:28,348 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:27:28,354 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:27:28,358 - ERROR - An error occurred while processing Düsseldorf - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:27:32,645 - INFO - Next cycle...\n",
      "2025-07-12 13:27:32,646 - INFO - Starting a new cycle of requests...\n",
      "2025-07-12 13:27:32,647 - INFO - Making API request for Duisburg HBF at 2025-07-12T13:27:32.647352\n",
      "2025-07-12 13:27:33,029 - INFO - (200) Request successful for Duisburg HBF at 2025-07-12T13:27:32.647352\n",
      "2025-07-12 13:27:33,031 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:27:33,044 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:27:33,049 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:27:33,053 - ERROR - An error occurred while processing Duisburg - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:27:37,339 - INFO - Making API request for Mönchengladbach HBF at 2025-07-12T13:27:37.339655\n",
      "2025-07-12 13:27:37,688 - INFO - (200) Request successful for Mönchengladbach HBF at 2025-07-12T13:27:37.339655\n",
      "2025-07-12 13:27:37,690 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:27:37,699 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:27:37,700 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:27:41,987 - INFO - Making API request for Wuppertal HBF at 2025-07-12T13:27:41.987447\n",
      "2025-07-12 13:27:42,355 - INFO - (200) Request successful for Wuppertal HBF at 2025-07-12T13:27:41.987447\n",
      "2025-07-12 13:27:42,356 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:27:42,366 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:27:42,367 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:27:46,654 - INFO - Making API request for Bochum HBF at 2025-07-12T13:27:46.654255\n",
      "2025-07-12 13:27:46,994 - INFO - (200) Request successful for Bochum HBF at 2025-07-12T13:27:46.654255\n",
      "2025-07-12 13:27:46,995 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:27:47,006 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:27:47,007 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:27:51,294 - INFO - Making API request for Dortmund HBF at 2025-07-12T13:27:51.294041\n",
      "2025-07-12 13:27:51,635 - INFO - (200) Request successful for Dortmund HBF at 2025-07-12T13:27:51.294041\n",
      "2025-07-12 13:27:51,637 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:27:51,648 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:27:51,649 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:27:55,936 - INFO - Making API request for Essen HBF at 2025-07-12T13:27:55.936410\n",
      "2025-07-12 13:27:56,453 - INFO - (200) Request successful for Essen HBF at 2025-07-12T13:27:55.936410\n",
      "2025-07-12 13:27:56,456 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:27:56,478 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:27:56,480 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:28:00,767 - INFO - Making API request for Düsseldorf HBF at 2025-07-12T13:28:00.767444\n",
      "2025-07-12 13:28:01,253 - INFO - (200) Request successful for Düsseldorf HBF at 2025-07-12T13:28:00.767444\n",
      "2025-07-12 13:28:01,255 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:28:01,272 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:28:01,278 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:28:01,282 - ERROR - An error occurred while processing Düsseldorf - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:28:05,569 - INFO - Next cycle...\n",
      "2025-07-12 13:28:05,570 - INFO - Starting a new cycle of requests...\n",
      "2025-07-12 13:28:05,571 - INFO - Making API request for Duisburg HBF at 2025-07-12T13:28:05.571566\n",
      "2025-07-12 13:28:05,985 - INFO - (200) Request successful for Duisburg HBF at 2025-07-12T13:28:05.571566\n",
      "2025-07-12 13:28:05,987 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:28:06,000 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:28:06,005 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:28:06,008 - ERROR - An error occurred while processing Duisburg - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:28:10,295 - INFO - Making API request for Mönchengladbach HBF at 2025-07-12T13:28:10.295219\n",
      "2025-07-12 13:28:11,720 - INFO - (200) Request successful for Mönchengladbach HBF at 2025-07-12T13:28:10.295219\n",
      "2025-07-12 13:28:11,722 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:28:11,734 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:28:11,739 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:28:11,744 - ERROR - An error occurred while processing Mönchengladbach - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n",
      "2025-07-12 13:28:16,030 - INFO - Making API request for Wuppertal HBF at 2025-07-12T13:28:16.030716\n",
      "2025-07-12 13:28:16,419 - INFO - (200) Request successful for Wuppertal HBF at 2025-07-12T13:28:16.030716\n",
      "2025-07-12 13:28:16,420 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:28:16,429 - INFO - No new UUIDs to append.\n",
      "2025-07-12 13:28:16,430 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:28:20,716 - INFO - Making API request for Bochum HBF at 2025-07-12T13:28:20.716650\n",
      "2025-07-12 13:28:21,179 - INFO - (200) Request successful for Bochum HBF at 2025-07-12T13:28:20.716650\n",
      "2025-07-12 13:28:21,181 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:28:21,194 - INFO - Loaded existing geodata with 7 entries. Columns: ['stop', 'line_x', 'direct_x', 'schedudep_', 'realdep_x', 'delay_x', 'line_y', 'direct_y', 'schedude_1', 'realdep_y', 'delay_y', 'line', 'direct', 'schedudep', 'realdep', 'delay', 'geometry']\n",
      "2025-07-12 13:28:21,200 - INFO - Loaded CSV with 200 entries. Columns: ['stop', 'line', 'direct', 'schedudep', 'realdep', 'delay']\n",
      "2025-07-12 13:28:21,203 - ERROR - An error occurred while processing Bochum - HBF: Passing 'suffixes' which cause duplicate columns {'realdep_x', 'line_x', 'delay_x', 'direct_x'} is not allowed.\n"
     ]
    }
   ],
   "source": [
    "delay_min = 0.5\n",
    "placename_list = [(\"Duisburg\", \"HBF\"), (\"Mönchengladbach\", \"HBF\"), (\"Wuppertal\", \"HBF\"), (\"Bochum\", \"HBF\"), (\"Dortmund\", \"HBF\"), (\"Essen\", \"HBF\"), (\"Düsseldorf\", \"HBF\")]\n",
    "\n",
    "main(delay_min, placename_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025_webkarto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
