{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b2666d7",
   "metadata": {},
   "source": [
    "# API Request Example to VRR\n",
    "Define the name of the API endpoint you want to query and the parameters you need to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d94a2472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import uuid\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize logging to log to both file and console\n",
    "if not os.path.exists('data/logs'):\n",
    "    os.makedirs('data/logs', exist_ok=True, parents=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('data/logs/api_requests.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_api_request(datetime_dt, place_dm, name_dm):\n",
    "\n",
    "    def communicate_response(response, place_dm, name_dm, datetime_dt):\n",
    "        \"\"\"Handles the response from the API and logs the status.\"\"\"\n",
    "        response_lut = { 200: \"Request successful\", 204: \"No departures found\", 400: \"Bad request\", 404: \"Not found\", 500: \"Internal server error\", 503: \"Service unavailable\", 429: \"Too many requests\"}\n",
    "        logging.info(f\"({response.status_code}) {response_lut.get(response.status_code, 'Unknown status')} for {place_dm} {name_dm} at {datetime_dt.isoformat()}\")\n",
    "        return response\n",
    "\n",
    "    def make_uid(stop, scheduled_datetime, line):\n",
    "        \"\"\"Generates a unique identifier for each departure based on stop, scheduled datetime, and line.\"\"\"\n",
    "        return str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"{stop}|{scheduled_datetime}|{line}\"))\n",
    "    \n",
    "    def build_results(datetime_dt, make_uid, departures):\n",
    "        \"\"\"Builds a list of dictionaries containing the departure information.\"\"\"\n",
    "        results = []\n",
    "        for dep in departures:\n",
    "            stop_name = dep.get('stopName')\n",
    "            platform = dep.get('platformName', dep.get('platform'))\n",
    "            scheduled = dep.get('dateTime', {})\n",
    "            real = dep.get('realDateTime', {})\n",
    "            \n",
    "            # Build full datetime for scheduled and real departure\n",
    "            try:\n",
    "                scheduled_dt = datetime(\n",
    "                    int(scheduled.get('year', datetime_dt.year)),\n",
    "                    int(scheduled.get('month', datetime_dt.month)),\n",
    "                    int(scheduled.get('day', datetime_dt.day)),\n",
    "                    int(scheduled.get('hour', 0)),\n",
    "                    int(scheduled.get('minute', 0))\n",
    "                )\n",
    "            except Exception:\n",
    "                scheduled_dt = None\n",
    "            try:\n",
    "                real_dt = datetime(\n",
    "                    int(real.get('year', datetime_dt.year)),\n",
    "                    int(real.get('month', datetime_dt.month)),\n",
    "                    int(real.get('day', datetime_dt.day)),\n",
    "                    int(real.get('hour', 0)),\n",
    "                    int(real.get('minute', 0))\n",
    "                ) if real else None\n",
    "            except Exception:\n",
    "                real_dt = None\n",
    "\n",
    "            line = dep.get('servingLine', {}).get('number')\n",
    "            direction = dep.get('servingLine', {}).get('direction')\n",
    "            delay = dep.get('servingLine', {}).get('delay')\n",
    "            cancelled = dep.get('servingLine', {}).get('cancelled')\n",
    "            connection_exists = not (str(cancelled) == \"1\")\n",
    "            # Additional fields\n",
    "            delay_reason = dep.get('servingLine', {}).get('delayReason')\n",
    "            realtime_status = dep.get('servingLine', {}).get('realtimeStatus')\n",
    "            status_text = dep.get('servingLine', {}).get('statusText')\n",
    "\n",
    "            uid = make_uid(stop_name, scheduled_dt, line)\n",
    "\n",
    "            results.append({\n",
    "                'uuid': uid,\n",
    "                'stop': stop_name,\n",
    "                'platform': platform,\n",
    "                'line': line,\n",
    "                'direction': direction,\n",
    "                'scheduled_departure': scheduled_dt,\n",
    "                'real_departure': real_dt,\n",
    "                'scheduled_time': scheduled_dt.time() if scheduled_dt else None,\n",
    "                'scheduled_date_iso': scheduled_dt.date().isoformat() if scheduled_dt else None,\n",
    "                'delay_min': int(delay) if delay not in (None, '', '-9999') else None,\n",
    "                'connection_exists': connection_exists,\n",
    "                'delay_reason': delay_reason,\n",
    "                'realtime_status': realtime_status,\n",
    "                'status_text': status_text\n",
    "            })\n",
    "            \n",
    "        return results\n",
    "    \n",
    "\n",
    "    # Prepare the parameters for the API request\n",
    "    params = {\n",
    "        \"language\": \"de\",\n",
    "        \"mode\": \"direct\",\n",
    "        \"outputFormat\": \"JSON\",\n",
    "        \"type_dm\": \"stop\",\n",
    "        \"useProxFootSearch\": 0,\n",
    "        \"useRealtime\": 1,\n",
    "        \"itdDateDay\": datetime_dt.day,\n",
    "        \"itdDateMonth\": datetime_dt.month,\n",
    "        \"itdDateYear\": datetime_dt.year,\n",
    "        \"itdTimeHour\": datetime_dt.hour,\n",
    "        \"itdTimeMinute\": datetime_dt.minute,\n",
    "        \"place_dm\": place_dm,\n",
    "        \"name_dm\": name_dm,\n",
    "    }\n",
    "\n",
    "    # Create a text file to store the raw API responses (Debugging purposes)\n",
    "    textfile = Path(\"vrr_api_full_responses.txt\")\n",
    "    if not textfile.exists():\n",
    "        textfile.touch()\n",
    "\n",
    "    # API URL for the VRR (Verkehrsverbund Rhein-Ruhr) departures\n",
    "    # This URL is used to fetch the departure information based on the parameters provided\n",
    "    # The API is expected to return a JSON response with the departure details\n",
    "    API_URL = \"https://efa.vrr.de/standard/XML_DM_REQUEST\"\n",
    "\n",
    "    # Make the API request\n",
    "    logging.info(f\"Making API request for {place_dm} {name_dm} at {datetime_dt.isoformat()}\")\n",
    "    response = requests.get(API_URL, params=params)\n",
    "    \n",
    "    # Handle the response\n",
    "    response = communicate_response(response, place_dm, name_dm, datetime_dt)\n",
    "\n",
    "    # Check if the response is successful and contains data\n",
    "    if response.status_code in [200, 204]:\n",
    "        # Write the raw response to a text file for debugging purposes\n",
    "        try:\n",
    "            with open(textfile, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(response.text + \"\\n\\n\")\n",
    "            logging.info(f\"Response written to {textfile}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error writing to {textfile}: {e}\")\n",
    "        data = response.json()\n",
    "    else:\n",
    "        # If the response is not successful, return an empty DataFrame and the status code\n",
    "        logging.error(f\"Failed to fetch data for {place_dm} {name_dm} at {datetime_dt.isoformat()}\")\n",
    "        raise requests.exceptions.RequestException(\n",
    "            f\"Request failed with status code {response.status_code} for {place_dm} {name_dm} at {datetime_dt.isoformat()}\"\n",
    "        )\n",
    "\n",
    "    # Extract the departure list from the response data\n",
    "    departures = data.get('departureList', [])\n",
    "\n",
    "    # Build the results from the departures\n",
    "    df_departures = pd.DataFrame(build_results(datetime_dt, make_uid, departures))\n",
    "\n",
    "    # LUT (Lookup Table) for replacing special characters in the stop, direction, and line names\n",
    "    # This is necessary to ensure that the data is clean and consistent, in this case for German characters\n",
    "    lut = {\"Ã¼\": \"ü\", \"Ã¶\": \"ö\", \"Ã¤\": \"ä\", \"ÃŸ\": \"ß\", \"Ã\": \"ß\"}\n",
    "    for col in ['stop', 'direction', 'line']:\n",
    "        df_departures[col] = df_departures[col].replace(lut, regex=True)\n",
    "\n",
    "    # Convert the scheduled and real departure times to ISO format\n",
    "    df_departures['scheduled_departure'] = pd.to_datetime(df_departures['scheduled_departure'], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    df_departures['real_departure'] = pd.to_datetime(df_departures['real_departure'], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    \n",
    "    # return df_departures, response.status_code\n",
    "    return df_departures, response.status_code\n",
    "\n",
    "def update_geodata(csv_file_path, geodata_file_path, geodata_target, n_data: int):\n",
    "    \"\"\"\n",
    "    For each stop in the DataFrame, get the last 10 departures and add them as lists into the GeoDataFrame.\n",
    "    The GeoDataFrame uses short column names due to shapefile limitations, so columns are mapped accordingly.\n",
    "    \"\"\"\n",
    "\n",
    "    row_load = n_data*20\n",
    "\n",
    "    # Load the CSV file into a DataFrame, get the last 200 rows\n",
    "    df = pd.read_csv(csv_file_path, usecols=[\n",
    "        'uuid', 'stop', 'platform', 'line', 'direction',\n",
    "        'scheduled_departure', 'real_departure', 'delay_min', 'connection_exists'\n",
    "    ]).tail(200)\n",
    "\n",
    "    # Load the geodata shapefile\n",
    "    gdf = gpd.read_file(geodata_file_path)\n",
    "\n",
    "    # Create a new DataFrame which will get the last 10 departures for each stop and put them into lists for each column so that only one row per stop is created\n",
    "    new_data = []\n",
    "    for stop in gdf['stop'].unique():\n",
    "        stop_data = df[df['stop'] == stop].sort_values(by='scheduled_departure').tail(n_data)\n",
    "        if not stop_data.empty:\n",
    "            # Create a dictionary for the stop with lists of the last 10 departures\n",
    "            new_data.append({\n",
    "                'stop': stop,\n",
    "                'departures': stop_data['uuid'].tolist(),\n",
    "                'platforms': stop_data['platform'].tolist(),\n",
    "                'lines': stop_data['line'].tolist(),\n",
    "                'directions': stop_data['direction'].tolist(),\n",
    "                'scheduled_departures': stop_data['scheduled_departure'].tolist(),\n",
    "                'real_departures': stop_data['real_departure'].tolist(),\n",
    "                'delays': stop_data['delay_min'].tolist(),\n",
    "                'connection_exists': stop_data['connection_exists'].tolist()\n",
    "            })\n",
    "\n",
    "    # Create a new GeoDataFrame from the new data\n",
    "    new_gdf = gpd.GeoDataFrame(new_data, geometry=gdf.geometry[gdf['stop'].isin([d['stop'] for d in new_data])].values, crs=gdf.crs)\n",
    "\n",
    "    # Save the new GeoDataFrame to the target geojson file\n",
    "    if not geodata_target.parent.exists():\n",
    "        geodata_target.parent.mkdir(parents=True)\n",
    "    new_gdf.to_file(geodata_target, driver='GeoJSON')\n",
    "\n",
    "\n",
    "# Main function to handle the API requests and manage the CSV file\n",
    "def main(delay_min, placename_list, n_entries):\n",
    "    total_requests = len(placename_list)\n",
    "    delay_s = delay_min * 60  # convert minutes to seconds\n",
    "    request_delay = delay_s / total_requests # time the actual requests so that they space out over the delay time\n",
    "\n",
    "    # initialize csv\n",
    "    csv_file = Path('final_departures.csv')\n",
    "    geodata_file = Path('res/geodata/bahnhoefe.shp')\n",
    "    geodata_target = Path('data/geodata/bahnhoefe_running.geojson')\n",
    "\n",
    "    # Main loop\n",
    "    logging.info(f\"Total requests: {total_requests}, Delay per request: {round(request_delay/60, 2)} minutes.\")\n",
    "    logging.info(\"Starting the request loop...\")\n",
    "\n",
    "    # Load existing UUIDs only once at the start\n",
    "    try:\n",
    "        existing_df = pd.read_csv(csv_file, usecols=['uuid'])\n",
    "        existing_uuids = set(existing_df['uuid'].dropna().astype(str))\n",
    "        logging.info(f\"Loaded {len(existing_uuids)} existing UUIDs.\")\n",
    "    except FileNotFoundError:\n",
    "        existing_uuids = set()\n",
    "        logging.info(\"No existing UUIDs found, starting fresh.\")\n",
    "\n",
    "    while True:\n",
    "        logging.info(\"Starting a new cycle of requests...\")\n",
    "\n",
    "        # Update the geodata with the new departures\n",
    "        try:\n",
    "            update_geodata(csv_file, geodata_file, geodata_target, n_entries)\n",
    "            logging.info(f\"Geodata updated and saved to {geodata_target}.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error updating geodata: {e}\")\n",
    "            time.sleep(request_delay)\n",
    "            continue\n",
    "\n",
    "        for place_dm, name_dm in placename_list:\n",
    "            \n",
    "            try:\n",
    "                datetime_dt = datetime.now()\n",
    "\n",
    "                df, status_code = full_api_request(datetime_dt, place_dm, name_dm)\n",
    "\n",
    "                if not df.empty:\n",
    "                    df['uuid'] = df['uuid'].astype(str)\n",
    "                    new_df = df[~df['uuid'].isin(existing_uuids)]\n",
    "\n",
    "                    if not new_df.empty:\n",
    "                        new_df.to_csv(csv_file, mode='a', header=not existing_uuids, index=False)\n",
    "                        existing_uuids.update(new_df['uuid'])\n",
    "\n",
    "                        logging.info(f\"Appended {len(new_df)} new departures. Status code: {status_code}\")\n",
    "                    else:\n",
    "                        logging.info(\"No new UUIDs to append.\")\n",
    "                else:\n",
    "                    logging.info(f\"No departures found for {place_dm} - {name_dm}. Status code: {status_code}\")\n",
    "\n",
    "                logging.info(f\"Sleeping for {round(request_delay/60, 2)} minutes.\")\n",
    "                time.sleep(request_delay)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logging.error(f\"Request failed for {place_dm} - {name_dm} ({status_code}): {e}\")\n",
    "                time.sleep(request_delay)\n",
    "                continue\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"An error occurred while processing {place_dm} - {name_dm}: {e}\")\n",
    "                time.sleep(request_delay)\n",
    "                continue\n",
    "\n",
    "        logging.info(\"Next cycle...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21eb1716",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (148001247.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# Define the datetime for the request\n",
    "return\n",
    "datetime_dt = datetime.now()\n",
    "\n",
    "# Define the place and name for the stop\n",
    "place_dm = \"Gelsenkirchen\"\n",
    "name_dm = \"HBF\"\n",
    "\n",
    "placename_list = [(\"Duisburg\", \"HBF\"), (\"Mönchengladbach\", \"HBF\"), (\"Wuppertal\", \"HBF\"), (\"Bochum\", \"HBF\"), (\"Dortmund\", \"HBF\"), (\"Essen\", \"HBF\"), (\"Düsseldorf\", \"HBF\")]\n",
    "placename_list = [(\"Duisburg\", \"HBF\")]\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "# Make the API request and get the DataFrame\n",
    "for place_dm, name_dm in placename_list:\n",
    "    print(f\"Requesting data for {place_dm} - {name_dm}\")\n",
    "    df, status_code = full_api_request(datetime_dt, place_dm, name_dm)\n",
    "    if not df.empty:\n",
    "        final_df = pd.concat([final_df, df], ignore_index=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35232bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 13:56:52,717 - INFO - Total requests: 7, Delay per request: 0.07 minutes.\n",
      "2025-07-12 13:56:52,719 - INFO - Starting the request loop...\n",
      "2025-07-12 13:56:52,724 - INFO - Loaded 1181 existing UUIDs.\n",
      "2025-07-12 13:56:52,725 - INFO - Starting a new cycle of requests...\n",
      "2025-07-12 13:56:52,752 - INFO - Created 7 records\n",
      "2025-07-12 13:56:52,753 - INFO - Geodata updated and saved to data/geodata/bahnhoefe_running.geojson.\n",
      "2025-07-12 13:56:52,754 - INFO - Making API request for Duisburg HBF at 2025-07-12T13:56:52.754243\n",
      "2025-07-12 13:56:53,057 - INFO - (200) Request successful for Duisburg HBF at 2025-07-12T13:56:52.754243\n",
      "2025-07-12 13:56:53,058 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:56:53,068 - INFO - Appended 3 new departures. Status code: 200\n",
      "2025-07-12 13:56:53,069 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:56:57,355 - INFO - Making API request for Mönchengladbach HBF at 2025-07-12T13:56:57.355692\n",
      "2025-07-12 13:56:57,641 - INFO - (200) Request successful for Mönchengladbach HBF at 2025-07-12T13:56:57.355692\n",
      "2025-07-12 13:56:57,642 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:56:57,653 - INFO - Appended 2 new departures. Status code: 200\n",
      "2025-07-12 13:56:57,654 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:57:01,941 - INFO - Making API request for Wuppertal HBF at 2025-07-12T13:57:01.941094\n",
      "2025-07-12 13:57:02,230 - INFO - (200) Request successful for Wuppertal HBF at 2025-07-12T13:57:01.941094\n",
      "2025-07-12 13:57:02,231 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:57:02,241 - INFO - Appended 2 new departures. Status code: 200\n",
      "2025-07-12 13:57:02,242 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:57:06,529 - INFO - Making API request for Bochum HBF at 2025-07-12T13:57:06.528925\n",
      "2025-07-12 13:57:06,850 - INFO - (200) Request successful for Bochum HBF at 2025-07-12T13:57:06.528925\n",
      "2025-07-12 13:57:06,852 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:57:06,866 - INFO - Appended 2 new departures. Status code: 200\n",
      "2025-07-12 13:57:06,867 - INFO - Sleeping for 0.07 minutes.\n",
      "2025-07-12 13:57:11,153 - INFO - Making API request for Dortmund HBF at 2025-07-12T13:57:11.153510\n",
      "2025-07-12 13:57:11,442 - INFO - (200) Request successful for Dortmund HBF at 2025-07-12T13:57:11.153510\n",
      "2025-07-12 13:57:11,443 - INFO - Response written to vrr_api_full_responses.txt\n",
      "2025-07-12 13:57:11,454 - INFO - Appended 6 new departures. Status code: 200\n",
      "2025-07-12 13:57:11,454 - INFO - Sleeping for 0.07 minutes.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m n_entries = \u001b[32m20\u001b[39m\n\u001b[32m      3\u001b[39m placename_list = [(\u001b[33m\"\u001b[39m\u001b[33mDuisburg\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHBF\u001b[39m\u001b[33m\"\u001b[39m), (\u001b[33m\"\u001b[39m\u001b[33mMönchengladbach\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHBF\u001b[39m\u001b[33m\"\u001b[39m), (\u001b[33m\"\u001b[39m\u001b[33mWuppertal\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHBF\u001b[39m\u001b[33m\"\u001b[39m), (\u001b[33m\"\u001b[39m\u001b[33mBochum\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHBF\u001b[39m\u001b[33m\"\u001b[39m), (\u001b[33m\"\u001b[39m\u001b[33mDortmund\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHBF\u001b[39m\u001b[33m\"\u001b[39m), (\u001b[33m\"\u001b[39m\u001b[33mEssen\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHBF\u001b[39m\u001b[33m\"\u001b[39m), (\u001b[33m\"\u001b[39m\u001b[33mDüsseldorf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHBF\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplacename_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_entries\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 266\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(delay_min, placename_list, n_entries)\u001b[39m\n\u001b[32m    263\u001b[39m         logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo departures found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplace_dm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname_dm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    265\u001b[39m     logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSleeping for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(request_delay/\u001b[32m60\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[32m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_delay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    269\u001b[39m     logging.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRequest failed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplace_dm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname_dm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "delay_min = 10\n",
    "n_entries = 20\n",
    "placename_list = [(\"Duisburg\", \"HBF\"), (\"Mönchengladbach\", \"HBF\"), (\"Wuppertal\", \"HBF\"), (\"Bochum\", \"HBF\"), (\"Dortmund\", \"HBF\"), (\"Essen\", \"HBF\"), (\"Düsseldorf\", \"HBF\")]\n",
    "\n",
    "main(delay_min, placename_list, n_entries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025_webkarto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
